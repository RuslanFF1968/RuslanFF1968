{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuslanFF1968/RuslanFF1968/blob/main/Clustering_Case.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN6pm2WNSppR"
      },
      "source": [
        "# Import of libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c-p9wyqmSKJ"
      },
      "source": [
        "from google.colab import files\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import utils\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout,SpatialDropout1D, BatchNormalization, Embedding, Flatten, Input\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdROn513s5bk",
        "outputId": "b03a93a6-d31f-4aa1-f205-0a76d3b5cf8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL25IskRnMJF"
      },
      "source": [
        "#Data uploading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WcL00HA6mpw",
        "outputId": "85495137-b640-4298-e2de-1b67be2c2550",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!rm -R /content/texts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/texts': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKVvVQJZ65HQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ae8b17e-b995-4a2d-de36-e422af960981"
      },
      "source": [
        "# Writing the path to the base in the Google Drive, creating the folder texts and extracting from there\n",
        "!unzip -q '/content/drive/My Drive/Базы/Тексты писателей.zip' -d /content/texts"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/My Drive/Базы/Тексты писателей.zip, /content/drive/My Drive/Базы/Тексты писателей.zip.zip or /content/drive/My Drive/Базы/Тексты писателей.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyboRqcV-UD0"
      },
      "source": [
        "def readText(fileName): # Declearing the functions for file reading. To the input the path to file is sent\n",
        "  f = open(fileName, 'r')        # Setting the opening of the desired file in the read mode\n",
        "  text = f.read()                # Reading the text\n",
        "  text = text.replace(\"\\n\", \" \") # Line breaks converting in spaces\n",
        "\n",
        "  return text                    # Returing the file text\n",
        "\n",
        "# Declaring the classes which are of interest\n",
        "className = [\"О. Генри\", \"Стругацкие\", \"Булгаков\", \"Саймак\", \"Фрай\", \"Брэдберри\"]\n",
        "nClasses = len(className) # Counting the number of classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83PUIzDNsvqY",
        "outputId": "f9b9d409-3739-41b8-a91d-c5c0cf6202fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5JSE4oQ3WbE",
        "outputId": "3733e913-d9ee-44d5-b570-8907882825f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "# Загружаем обучающие тексты\n",
        "\n",
        "trainText = [] # Forming the training texts\n",
        "testText = []  # Forming the test texts\n",
        "\n",
        "# The formation shall be implemented as follows\n",
        "# The class of each i-th element in the training sampling must correspond to\n",
        "# to the class of each i-th element in the test sampling\n",
        "\n",
        "for i in className:              # Go through each class\n",
        "  for j in os.listdir('texts/'): # Go through each file in the folder with texts #\n",
        "    if i in j:                   # Checking if the file j contains the name of the class i in the name\n",
        "\n",
        "      if 'Обучающая' in j:                       # Если в имени найденного класса есть строка \"Обучающая\"\n",
        "        trainText.append(readText('texts/' + j)) # добавляем в обучающую выборку\n",
        "        print(j, 'добавлен в обучающую выборку') # Выводим информацию\n",
        "      if 'Тестовая' in j:                        # Если в имени найденного класса есть строка \"Тестовая\"\n",
        "        testText.append(readText('texts/' + j))  # добавляем в обучающую выборку\n",
        "        print(j, 'добавлен в тестовую выборку')  # Выводим информацию\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'texts/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-7619a1fb605e>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m:\u001b[0m              \u001b[0;31m# Проходим по каждому классу\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'texts/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Проходим по каждому файлу в папке с текстами #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m                   \u001b[0;31m# Проверяем, содержит ли файл j в названии имя класса i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'texts/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYwujAuLoO5s"
      },
      "source": [
        "print(len(trainText))    # Количество элементов в trainText\n",
        "print(len(trainText[0])) # Количество символов в первом источнике в trainText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BYd_uRgTQ_0"
      },
      "source": [
        "#Обработка данных\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCxn8LYv3glJ"
      },
      "source": [
        "maxWordsCount = 30000 # Определяем максимальное количество слов/индексов, учитываемое при обучении текстов\n",
        "\n",
        "# Воспользуемся встроенной в Keras функцией Tokenizer для разбиения текста и превращения в матрицу числовых значений\n",
        "# num_words=maxWordsCount - определяем максимальное количество слов/индексов, учитываемое при обучении текстов\n",
        "# filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n' - избавляемся от ненужных символов\n",
        "# lower=True - приводим слова к нижнему регистру\n",
        "# split=' ' - разделяем слова по пробелу\n",
        "# char_level=False - просим токенайзер не удалять однобуквенные слова\n",
        "tokenizer = Tokenizer(num_words=maxWordsCount, filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff', lower=True, split=' ', oov_token='unknown', char_level=False)\n",
        "\n",
        "tokenizer.fit_on_texts(trainText) # \"Скармливаем\" наши тексты, т. е. даём в обработку методу, который соберет словарь частотности\n",
        "items = list(tokenizer.word_index.items()) # Вытаскиваем индексы слов для просмотра"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgiANDRE_tWF"
      },
      "source": [
        "print(items[:50])                   # Посмотрим 50 самых часто встречающихся слов\n",
        "print(\"Размер словаря\", len(items)) # Длина словаря"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkJeOizQABT6"
      },
      "source": [
        "# Преобразовываем текст в последовательность индексов согласно частотному словарю\n",
        "trainWordIndexes = tokenizer.texts_to_sequences(trainText) # Обучающие тесты в индексы\n",
        "testWordIndexes = tokenizer.texts_to_sequences(testText)   # Проверочные тесты в индексы\n",
        "\n",
        "print(\"Взглянем на фрагмент обучающего текста:\")\n",
        "print(\"В виде оригинального текста:              \", trainText[1][:87])\n",
        "print(\"Он же в виде последовательности индексов: \", trainWordIndexes[1][:20], '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apxVpC19sJxq"
      },
      "source": [
        "print(\"Статистика по обучающим текстам:\")\n",
        "\n",
        "symbolsTrainText = 0 # Объявляем переменную для подсчета символов в обучающих текстах\n",
        "wordsTrainText = 0   # Объявляем переменную для подсчета слов в обучающих текстах\n",
        "\n",
        "for i in range(nClasses): # Проходим по всем классам\n",
        "  print(className[i], \" \"*(10-len(className[i])), len(trainText[i]), \"символов, \", len(trainWordIndexes[i]), \"слов\")\n",
        "  symbolsTrainText += len(trainText[i])      # Считаем символы\n",
        "  wordsTrainText += len(trainWordIndexes[i]) # Считаем слова\n",
        "\n",
        "print('----')\n",
        "print(\"В сумме \", symbolsTrainText, \" символов, \", wordsTrainText, \" слов \\n\")\n",
        "print()\n",
        "print(\"Статистика по тестовым текстам:\")\n",
        "\n",
        "symbolsTestText = 0 # Объявляем переменную для подсчета символов в тестовых текстах\n",
        "wordsTestText = 0   # Объявляем переменную для подсчета слов в тестовых текстах\n",
        "\n",
        "for i in range(nClasses): # Проходим по всем классам\n",
        "  print(className[i], ' '*(10-len(className[i])), len(testText[i]), \"символов, \", len(testWordIndexes[i]), \"слов\")\n",
        "  symbolsTestText += len(testText[i])      # Считаем символы\n",
        "  wordsTestText += len(testWordIndexes[i]) # Считаем слова\n",
        "print('----')\n",
        "print(\"В сумме \", symbolsTestText, \" символов, \", wordsTestText, \" слов\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slCp7AkSw0Oo"
      },
      "source": [
        "#Создание обучающей и проверочной выборки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzgY9gNMRRk6"
      },
      "source": [
        "**Функции для формирования выборки по отрезкам текста с заданным шагом**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP69cPxWdHq3"
      },
      "source": [
        "###########################\n",
        "# Формирование обучающей выборки по листу индексов слов\n",
        "# (разделение на короткие векторы)\n",
        "##########################\n",
        "def getSetFromIndexes(wordIndexes, xLen, step): # wordIndexes - список индексов, xLen - длина отрезка разбиения, step - шаг разбиения\n",
        "  xSample = []                # Создаем пустой список под результат\n",
        "  wordsLen = len(wordIndexes) # Получаем длину списка индексов\n",
        "  index = 0 # Задаем переменную, в которой будет храниться стартовый индекс для очередного \"куска\" разбиения\n",
        "\n",
        "  #Идём по всей длине вектора индексов\n",
        "  #\"Откусываем\" векторы длины xLen и смещаемся вперёд на step\n",
        "\n",
        "  while (index + xLen <= wordsLen):               # Если можно получить очередной \"кусок\"\n",
        "    xSample.append(wordIndexes[index:index+xLen]) # Добавляем в результирующий список элементы из wordIndexes, начиная с элемента с индексом index, заканчивая индексом index+xLen\n",
        "    index += step                                 # Увеличиваем index на величину шага\n",
        "\n",
        "# Таким образом на 0 итерации в результат добавятся элементы с 0 по 3000 (index=0, index + xLen = 3000)\n",
        "# На первой итерации index увеличиваем на 100 и врезультат добавятся элементы с 100 по 1100 (index = 100, index + xLen = 1100)\n",
        "# И так до тех пор, пока index + xLen не станет больше длины входного списка индексов wordsLen\n",
        "\n",
        "  return xSample # Возвращаем сформированный список"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnVyegrxcnZM"
      },
      "source": [
        "###########################\n",
        "# Формирование обучающей и проверочной выборки\n",
        "# Из двух листов индексов от двух классов\n",
        "##########################\n",
        "def createSetsMultiClasses(wordIndexes, xLen, step): # wordIndexes - список индексов для всех классов, xLen - длина отрезка разбиения, step - шаг разбиения\n",
        "\n",
        "  # Для каждого из 6 классов\n",
        "  # Создаём обучающую/проверочную выборку из индексов\n",
        "  nClasses = len(wordIndexes) # Задаем количество классов выборки\n",
        "  classesXSamples = []        # Создаем пустой список под выборки для каждого из классов\n",
        "  for wI in wordIndexes:      # для каждого текста выборки из последовательности индексов\n",
        "    classesXSamples.append(getSetFromIndexes(wI, xLen, step)) # добавляем в список очередную выборку\n",
        "  # В итоге classesXSamples будет иметь nClasses-элементов, где каждый элемент представляет собой список, который возвращает функция getSetFromIndexes\n",
        "\n",
        "  # Формируем один общий xSamples\n",
        "  x_train = [] # Создаем пустой список под x_train\n",
        "  y_train = [] # Создаем пустой список под y_train\n",
        "\n",
        "  for t in range(nClasses):   # Проходим по всем имеющимся классам\n",
        "    currY = utils.to_categorical(t, nClasses) # Tекущий класс переводится в вектор длиной 6 вида [0.0.0.1.0.0.]\n",
        "    xT = classesXSamples[t]   # Берем выборку для очередного класса\n",
        "    for i in range(len(xT)):  # Проходим по всем элементам выборки\n",
        "      x_train.append(xT[i])   # Добавляем в x_train\n",
        "      y_train.append(currY)   # Добавляем соответствующий вектор класса\n",
        "\n",
        "  x_train = np.array(x_train) # Переводим в массив numpy для подачи в нейронку\n",
        "  y_train = np.array(y_train) # Переводим в массив numpy для подачи в нейронку\n",
        "\n",
        "  return (x_train, y_train)   # Функция возвращает выборку и соответствующие векторы классов"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pi3Up8OvTtr"
      },
      "source": [
        "**Подготовка данных**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgzoAiyawSwJ"
      },
      "source": [
        "# Задаём базовые параметры\n",
        "xLen = 3000 # Длина отрезка текста, по которой анализируем, в словах\n",
        "step = 100  # Шаг разбиения исходного текста на обучающие вектора"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqAgZWT8RH0b"
      },
      "source": [
        "# Формируем обучающую и тестовую выборку\n",
        "xTrain, yTrain = createSetsMultiClasses(trainWordIndexes, xLen, step) # Извлекаем обучающую выборку\n",
        "xTest, yTest = createSetsMultiClasses(testWordIndexes, xLen, step)    # Извлекаем тестовую выборку\n",
        "\n",
        "# Отображаем размерности\n",
        "print(xTrain.shape)\n",
        "print(yTrain.shape)\n",
        "print(xTest.shape)\n",
        "print(yTest.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gI1hXsRmhUo"
      },
      "source": [
        "**Recognition of the test sampling**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stHRW8-4mf7G"
      },
      "source": [
        "###########################\n",
        "# Представляем тестовую выборку в удобных для распознавания размерах\n",
        "##########################\n",
        "def createTestMultiClasses(wordIndexes, xLen, step): #функция принимает последовательность индексов, размер окна, шаг окна\n",
        "  # Для каждого из 6 классов\n",
        "  # Создаём тестовую выборку из индексов\n",
        "  nClasses = len(wordIndexes) # Получаем количество классов\n",
        "  xTest6Classes = []          # Создаем пустой список под результат\n",
        "  for wI in wordIndexes:      # Проходим по всем текстам из входного набора wordIndexes\n",
        "    sample = (getSetFromIndexes(wI, xLen, step)) # Разбиваем очерденой текст на отрезки длиной xLen с шагом step\n",
        "    xTest6Classes.append(np.array(sample))       # Добавляем полученный список к результату\n",
        "  return xTest6Classes        # Возвращаем сформированный список\n",
        "\n",
        "###########################\n",
        "# Распознаём тестовую выборку и выводим результаты\n",
        "##########################\n",
        "def recognizeMultiClass(model, xTest, modelName):\n",
        "  print(\"НЕЙРОНКА: \", modelName) # Отображаем имя нейронки\n",
        "  print()\n",
        "\n",
        "  val = []  # Создаем пустой список под результат\n",
        "  pred = [] # Создаем пустой список для хранений распознанных значений отдельных \"кусков\"\n",
        "\n",
        "  totalSumRec = [] # Создаем пустой список под результат\n",
        "\n",
        "  for i in range(nClasses):            # Проходим по всем классам\n",
        "    currPred = model.predict(xTest[i]) # Распознаем i-тый элемент тестовой выборки (xTest[i] - список, который создается функцией createTestMultiClasses для i-го тестового набора)\n",
        "    pred.append(currPred)              # Добавляем распознанный список значений в pred\n",
        "    currOut = np.argmax(currPred, axis=1) # Опрделяем номер распознаного класса для каждого элемента из currPred\n",
        "\n",
        "    # Считаем процент распознавания каждого класса за каждый\n",
        "    # Получаем матрицу 6 на 6\n",
        "    evVal = []\n",
        "    sumRec = []\n",
        "    for j in range(nClasses): # Проходим по всем классам\n",
        "      sumRec.append(0)\n",
        "      for t in range(len(currOut)): # Проходим по всему количество элементу в currOut (равно количество кусков, на которые был разделен исходных тестовый набор)\n",
        "        if (currOut[t] == j):       # Если currOut[t] (класс, к которому модель отнесла данный кусок) равен текущему классу j\n",
        "          sumRec[j] += 1            # То увеличиваем на 1 сумму sumRec для текущего класса\n",
        "      evVal.append(sumRec[j] / len(currOut)) # Считаем evVel для текущего класса (количечство элеметнов, которые были отнесены к классу j делим на общее количество элементов)\n",
        "\n",
        "    # В итоге evVal будет длиной равной количеству классу, где i-тый элемент показывает вероятность принадлежности к i-му классу\n",
        "    # Например evVal = [0.79, 0.02, 0.13, 0.05, 0.03, 0.1] означает, что 79% всех элементов в xTest[i] были отнесены к первому классу писателей и т. д.\n",
        "    totalSumRec.append(sumRec[i])   # Добавляем в totalSumRec количество вернораспознанных элементов sumRec[i] для i-го класса\n",
        "\n",
        "    # Определяем, какой в класс в итоге за какой был распознан\n",
        "    val.append(evVal[i]) # Добавляем процент для i-го класса из evVal в массив val\n",
        "    recognizedClass = np.argmax(evVal) # Находим индекс максимального элемента в evVal (соответсвует номеру класса)\n",
        "\n",
        "    # Выводим результаты распознавания по текущему классу\n",
        "    isRecognized = \"распознано НЕ ВЕРНО!\" # По умолчанию \"неверно\"\n",
        "    if (recognizedClass == i):            # Если индекс максимального элемента в evVal равен текущему классу i, то\n",
        "      isRecognized = \"распознано ВЕРНО!\"  # распознано верно\n",
        "    print(className[i], \" распознано \", int(100*evVal[i]), \"% сеть считает, что это \", className[recognizedClass], \", \", isRecognized) # Выводим информацию о распознавании\n",
        "\n",
        "  # Выводим средний процент распознавания по всем классам вместе\n",
        "  print()\n",
        "  sumVal = 0\n",
        "  sumCount = 0\n",
        "  lenXtest = []\n",
        "  for i in range(nClasses):\n",
        "    sumCount += len(xTest[i]) # Увеличиваем общее количество элементов на длину len(xTest[i])\n",
        "    sumVal += totalSumRec[i]  # Увеличиваем количество верных ответов на totalSumRec[i]\n",
        "  print(\"Средний процент распознавания \", int(100*sumVal/sumCount), \"%\") # Отображаем средний процент для всех примеров в тестовой выборке\n",
        "\n",
        "  print()\n",
        "\n",
        "  return sumVal/sumCount # Возвращаем средний процент распознавания"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "674DqQGgm9qn"
      },
      "source": [
        "#Embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKVdzykWzBO-"
      },
      "source": [
        "**Embedding + Dense**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHtgpvoLYYzA"
      },
      "source": [
        "# Создаём сеть с Embedding слоем\n",
        "text_input = Input((xTrain.shape[1],)) # Задаем входной слой для нейронки (размерность входных данных равна xTrain.shape[1] (в нашем примере 3000))\n",
        "emb = Embedding(maxWordsCount, 100, input_length=xLen)(text_input) # Добавляем Embedding-слой с размерностью векторного прстранства 100 (выбрано тестовым путем) и сохраняем в переменнуд emb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL9gl8NMTMCj"
      },
      "source": [
        "# Создаем стандартную полносвязную сеть для классификатора\n",
        "x = SpatialDropout1D(0.25)(emb)\n",
        "x = Flatten()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(200, activation=\"relu\")(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = BatchNormalization()(x)\n",
        "output = Dense(6, activation='softmax')(x) # Добавляем Dense-слой с softmax-Активацией и 6 нейронами\n",
        "\n",
        "modelE = Model(text_input, output) # Создаем модель классификатора со входм text_input и выходом output\n",
        "\n",
        "# Компилируем модель\n",
        "modelE.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Обучаем сеть на xTrain-наборе\n",
        "history = modelE.fit(xTrain,\n",
        "                    yTrain,\n",
        "                    epochs=30,\n",
        "                    batch_size=200,\n",
        "                    validation_data=(xTest, yTest))\n",
        "\n",
        "# Отображаем графики для acc и vall_acc\n",
        "plt.plot(history.history['accuracy'],\n",
        "         label='Доля верных ответов на обучающем наборе')\n",
        "plt.plot(history.history['val_accuracy'],\n",
        "         label='Доля верных ответов на проверочном наборе')\n",
        "plt.xlabel('Эпоха обучения')\n",
        "plt.ylabel('Доля верных ответов')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRXyitdIonei"
      },
      "source": [
        "modelE.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVk8YstQBbWG"
      },
      "source": [
        "# Проверяем результаты работы Ebedding сети\n",
        "xTest6Classes = createTestMultiClasses(testWordIndexes, xLen, step)\n",
        "# Отобразим размерности тестовых текстов, разбитых на куски фиксированной длины\n",
        "for i in range(nClasses):\n",
        "  print(str(len(xTest6Classes[i])) + ' x ' + str(len(xTest6Classes[i][0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqDRjTRkBFkF"
      },
      "source": [
        "# Отобразим статистику распознавания по писателям\n",
        "pred = recognizeMultiClass(modelE, xTest6Classes, \"Embedding + Dense\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIVYSz0mrt_T"
      },
      "source": [
        "#Кластеризация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcSEdQ6oUoIL"
      },
      "source": [
        "# Создадим модель, которая на входе дает данные с embedding'а\n",
        "model2 = Model(text_input, emb)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfPXVQjhs4Zk"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR_d_g5xs6Rg"
      },
      "source": [
        "# Назначим нашей модели веса из обученной ранее модели классификатора\n",
        "model2.layers[0] = modelE.layers[0] # Веса входного слоя нашей модели соответствуют весам входного слоя модели классификатора\n",
        "model2.layers[1] = modelE.layers[1] # Веса Embedding слоя нашей модели соответствуют весам Embedding слоя модели классификатора"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qdDs5WGU8OF"
      },
      "source": [
        "allwords = np.array([it[0] for it in items])   # Создадим список слов из нашего словаря items\n",
        "allindexes = np.array([it[1] for it in items]) # Создадим список индексов из нашего словаря items"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-XTkpWWVgGL"
      },
      "source": [
        "print(allwords[:20])   # Отобразим первый 20 слов\n",
        "print(allindexes[:20]) # Отобразим первые 20 индексов"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqJTqArvueMZ"
      },
      "source": [
        "len(allwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK1sGxj6Yj0O"
      },
      "source": [
        "# Так как наша модель требует входные данные размерностью 3000\n",
        "# Требуется преобразовать массив индексов allindexes таким образом, чтобы длина каждого из его элементов была равна 3000\n",
        "\n",
        "# Один из способов реализовать это: размножить каждый элемент 3000 раз\n",
        "words = np.array([np.array([allindexes[t] for i in range(3000)]) for t in range(5000)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Zye4IeuY5J4"
      },
      "source": [
        "print(words.shape) # Отобразим размерность words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDxgG70saHN4"
      },
      "source": [
        "print(words[0, :20]) # Отобразим первые 20 элементов 0го элемента\n",
        "print(words[1, :20]) # Отобразим первые 20 элементов 1го элемента"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npI_zueZZEj2"
      },
      "source": [
        "outall = model2.predict(words) # Теперь можно вызывать predict для массива words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IirI8yRdZQ3F"
      },
      "source": [
        "print(outall.shape) # Отобразим размерность outall"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5M5BzkLZVcS"
      },
      "source": [
        "# Так как в words все значения одной строки одинаковые (1,1,1,1... или 2,2,2,2.... и т.д.)\n",
        "# То и outall все значения одной строки будут одинаковыми\n",
        "# Поэтому без проблем можем оставить только 0-ые элементы из outall\n",
        "outall2 = np.array([o[0] for o in outall])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-RxTlPvZf2E"
      },
      "source": [
        "print(outall2.shape) # Выведем размерность outall2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDIUPpk5ZhAb"
      },
      "source": [
        "print(outall2[0]) # Отобразим данные 0го элемента\n",
        "# Они представляют из себя 100мерный вектор\n",
        "# Этот вектор представляет собой закодированное слово \"и\" (1-е слово в нашем словаре)\n",
        "# Во втором элементе будет храниться закодированное 100мерным вектором 2-е слово в словаре: \"в\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k8BMU4XcIZ9"
      },
      "source": [
        "from sklearn.cluster import KMeans # Подключаем модуль Kmeans\n",
        "import time # Подключаем модуль time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uutSTstcI8R"
      },
      "source": [
        "current_time = time.time()    # Засекаем текщуее время начала операции\n",
        "clustersCount = 100           # Задаем количество кластеров (выбрано произвольным путем из расчета того, что всего 20 000 слов берем)\n",
        "kmean = KMeans(clustersCount) # Создаем экземпляр KMeans и указываем количество классов\n",
        "kmean.fit(outall2)            # Делаем обработку по Embedding-данным из outall2\n",
        "labels = kmean.labels_        # Запоминаем метки классов в labels\n",
        "print ('Время обработки:' + str(round(time.time() - current_time)) + 'с.') # Выводим время работы алгоритма"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIyc9UoLckvA"
      },
      "source": [
        "print(labels[:20]) # Отобразим первые 20 меток"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmIVSsZ_cvI3"
      },
      "source": [
        "start_allwords = allwords[:5000] # Возьмем первые 5 000 слов из списка всех слов allwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_lR1J3dcpPh"
      },
      "source": [
        "print(start_allwords[labels==1]) # Отобразим, какие слова из start_allwords попали в 1 кластер"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uObxS6-wvNm"
      },
      "source": [
        "# Выведем 100 кластеров\n",
        "for i in range(100):\n",
        "  print(i, start_allwords[labels==i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aQcTE8LYCda"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}